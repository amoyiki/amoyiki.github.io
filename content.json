[{"title":"Scrapy学习（四） 爬取微博数据","date":"2017-02-13T06:48:00.000Z","path":"2017/02/13/Scrapy学习（四）-爬取微博数据/","text":"前言 Scrapy学习（三） 爬取豆瓣图书信息 接上篇之后。这次来爬取需要登录才能访问的微博。爬虫目标是获取用户的微博数、关注数、粉丝数。为建立用户关系图(尚未实现)做数据储备 准备 安装第三方库requests和pymongo 安装MongoDB 创建一个weibo爬虫项目 如何创建Scrapy项目之前文章都已经提到了，直接进入主题。 创建ItemsItem数据这部分我只需要个人信息，微博数，关注数、分数数这些基本信息就行。1234567891011121314151617181920212223242526272829class ProfileItem(Item): \"\"\" 账号的微博数、关注数、粉丝数及详情 \"\"\" _id = Field() nick_name = Field() profile_pic = Field() tweet_stats = Field() following_stats = Field() follower_stats = Field() sex = Field() location = Field() birthday = Field() bio = Field() class FollowingItem(Item): \"\"\" 关注的微博账号 \"\"\" _id = Field() relationship = Field()class FollowedItem(Item): \"\"\" 粉丝的微博账号 \"\"\" _id = Field() relationship = Field() 编写Spider为了方便爬虫，我们选择登陆的入口是手机版的微博http://weibo.cn/。 其中微博的uid可以通过访问用户资料页或者从关注的href属性中获取 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class WeiboSpiderSpider(scrapy.Spider): name = \"weibo_spider\" allowed_domains = [\"weibo.cn\"] url = \"http://weibo.cn/\" start_urls = ['2634877355',...] # 爬取入口微博ID task_set = set(start_urls) # 待爬取集合 tasked_set = set() # 已爬取集合 ... def start_requests(self): while len(self.task_set) &gt; 0 : _id = self.task_set.pop() if _id in self.tasked_set: raise CloseSpider(reason=\"已存在该数据 %s \"% (_id) ) self.tasked_set.add(_id) info_url = self.url + _id info_item = ProfileItem() following_url = info_url + \"/follow\" following_item = FollowingItem() following_item[\"_id\"] = _id following_item[\"relationship\"] = [] follower_url = info_url + \"/fans\" follower_item = FollowedItem() follower_item[\"_id\"] = _id follower_item[\"relationship\"] = [] yield scrapy.Request(info_url, meta=&#123;\"item\":info_item&#125;, callback=self.account_parse) yield scrapy.Request(following_url, meta=&#123;\"item\":following_item&#125;, callback=self.relationship_parse) yield scrapy.Request(follower_url, meta=&#123;\"item\":follower_item&#125;, callback=self.relationship_parse) def account_parse(self, response): item = response.meta[\"item\"] sel = scrapy.Selector(response) profile_url = sel.xpath(\"//div[@class='ut']/a/@href\").extract()[1] counts = sel.xpath(\"//div[@class='u']/div[@class='tip2']\").extract_first() item['_id'] = re.findall(u'^/(\\d+)/info',profile_url)[0] item['tweet_stats'] = re.findall(u'微博\\[(\\d+)\\]', counts)[0] item['following_stats'] = re.findall(u'关注\\[(\\d+)\\]', counts)[0] item['follower_stats'] = re.findall(u'粉丝\\[(\\d+)\\]', counts)[0] if int(item['tweet_stats']) &lt; 4500 and int(item['following_stats']) &gt; 1000 and int(item['follower_stats']) &lt; 500: raise CloseSpider(\"僵尸粉\") yield scrapy.Request(\"http://weibo.cn\"+profile_url, meta=&#123;\"item\": item&#125;,callback=self.profile_parse) def profile_parse(self,response): item = response.meta['item'] sel = scrapy.Selector(response) info = sel.xpath(\"//div[@class='tip']/following-sibling::div[@class='c']\").extract_first() item[\"profile_pic\"] = sel.xpath(\"//div[@class='c']/img/@src\").extract_first() item[\"nick_name\"] = re.findall(u'昵称:(.*?)&lt;br&gt;',info)[0] item[\"sex\"] = re.findall(u'性别:(.*?)&lt;br&gt;',info) and re.findall(u'性别:(.*?)&lt;br&gt;',info)[0] or '' item[\"location\"] = re.findall(u'地区:(.*?)&lt;br&gt;',info) and re.findall(u'地区:(.*?)&lt;br&gt;',info)[0] or '' item[\"birthday\"] = re.findall(u'生日:(.*?)&lt;br&gt;',info) and re.findall(u'生日:(.*?)&lt;br&gt;',info)[0] or '' item[\"bio\"] = re.findall(u'简介:(.*?)&lt;br&gt;',info) and re.findall(u'简介:(.*?)&lt;br&gt;',info)[0] or '' yield item def relationship_parse(self, response): item = response.meta[\"item\"] sel = scrapy.Selector(response) uids = sel.xpath(\"//table/tr/td[last()]/a[last()]/@href\").extract() new_uids = [] for uid in uids: if \"uid\" in uid: new_uids.append(re.findall('uid=(\\d+)&amp;',uid)[0]) else: try: new_uids.append(re.findall('/(\\d+)', uid)[0]) except: print('--------',uid) pass item[\"relationship\"].extend(new_uids) for i in new_uids: if i not in self.tasked_set: self.task_set.add(i) next_page = sel.xpath(\"//*[@id='pagelist']/form/div/a[text()='下页']/@href\").extract_first() if next_page: yield scrapy.Request(\"http://weibo.cn\"+next_page, meta=&#123;\"item\": item&#125;,callback=self.relationship_parse) else: yield item 代码中值得注意的地方有几个。 start_url这里我们填写的是微博的uid，有的用户有自定义域名（如上图），要访问后才能得到真正的uidstart_url 填写的初始种子数要在10个以上。这是为了确保后面我们爬取到的新的种子能够加入到待爬取的队列中。10个以上的规定是从Scrapy文档中查得的 ####REACTOR_THREADPOOL_MAXSIZE###Default: 10线程数是Twisted线程池的默认大小(The maximum limit for Twisted Reactor thread pool size.) CloseSpider当遇到不需要的继续爬取的连接时(如已经爬取过的链接，定义的僵尸粉链接等等)，就可以用CloseSpider关闭当前爬虫线程 编写middlewares123456class CookiesMiddleware(object): \"\"\" 换Cookie \"\"\" def process_request(self, request, spider): cookie = random.choice(cookies) request.cookies = cookie 编写cookie的获取方法这里我原本是想用手机版的微博去模拟登陆的，奈何验证码是在是太难搞了。所以我直接用网上有人编写好的登陆网页版微博的代码SinaSpider 这位写的很好，有兴趣的可以去看看。其中还有另一位写了模拟登陆（带验证码） 经测试可用。只不过我还没想好怎么嵌入到我的项目中。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# encoding=utf-8import jsonimport base64import requestsmyWeiBo = [ &#123;'no': 'xx@sina.com', 'psw': 'xx'&#125;, &#123;'no': 'xx@qq.com', 'psw': 'xx'&#125;,]def getCookies(weibo): \"\"\" 获取Cookies \"\"\" cookies = [] loginURL = r'https://login.sina.com.cn/sso/login.php?client=ssologin.js(v1.4.15)' for elem in weibo: account = elem['no'] password = elem['psw'] username = base64.b64encode(account.encode('utf-8')).decode('utf-8') postData = &#123; \"entry\": \"sso\", \"gateway\": \"1\", \"from\": \"null\", \"savestate\": \"30\", \"useticket\": \"0\", \"pagerefer\": \"\", \"vsnf\": \"1\", \"su\": username, \"service\": \"sso\", \"sp\": password, \"sr\": \"1440*900\", \"encoding\": \"UTF-8\", \"cdult\": \"3\", \"domain\": \"sina.com.cn\", \"prelt\": \"0\", \"returntype\": \"TEXT\", &#125; session = requests.Session() r = session.post(loginURL, data=postData) jsonStr = r.content.decode('gbk') info = json.loads(jsonStr) if info[\"retcode\"] == \"0\": print(\"Get Cookie Success!( Account:%s )\" % account) cookie = session.cookies.get_dict() cookies.append(cookie) else: print(\"Failed!( Reason:%s )\" % info[\"reason\"].encode(\"utf-8\")) return cookiescookies = getCookies(myWeiBo) 登陆-反爬虫的这部分应该是整个项目中最难的地方了。好多地方我都还不太懂。以后有空在研究 编写pipelines这边只需要主要什么类型的Item存到那张表里就行了12345678910111213141516171819202122class MongoDBPipeline(object): def __init__(self): connection = MongoClient( host=settings['MONGODB_SERVER'], port=settings['MONGODB_PORT'] ) db = connection[settings['MONGODB_DB']] self.info = db[settings['INFO']] self.following = db[settings['FOLLOWING']] self.followed = db[settings['FOLLOWED']] def process_item(self, item, spider): if isinstance(item, ProfileItem): self.info.insert(dict(item)) elif isinstance(item, FollowingItem): self.following.insert(dict(item)) elif isinstance(item, FollowedItem): self.followed.insert(dict(item)) log.msg(\"Weibo added to MongoDB database!\", level=log.DEBUG, spider=spider) return item 运行一下程序，就能看到MongoDB中有了我们要的数据了 总结 settings中的DOWNLOAD_DELAY设置5才能防止被微博BAN掉 尝试在利用cookies登陆失败时使用模拟登陆，但是效果很不理想 尝试用代理IP池反爬虫，但是尝试失败主要是不太会 未来将利用D3.js将爬到的数据绘制出来(或许吧) 项目地址：weibo_spider","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"},{"name":"Scrapy","slug":"Scrapy","permalink":"http://amoyiki.com/tags/Scrapy/"}]},{"title":"Scrapy学习（三） 爬取豆瓣图书信息","date":"2017-02-04T07:07:08.000Z","path":"2017/02/04/Scrapy学习（三）-爬取豆瓣图书信息/","text":"前言 Scrapy学习（一） 安装 Scrapy学习（二） 入门 有了前两篇的基础，就可以开始互联网上爬取我们感兴趣的信息了。因为暂时还没有学到如何模拟登陆，所以我就先抓像豆瓣这样不需要登陆的网站上的内容。我的开发环境是 Win7 + PyChram + Python3.5 + MongoDB爬虫的目标是豆瓣的日本文学标签下的所有书籍基本信息 创建项目 scrapy startproject douban 接着移动到douban目录下 scrapy genspider book book.douban.com 在spider目录下生成相应的BookSpider模板 编写Item在items.py中编写我们需要的数据模型12345678class BookItem(scrapy.Item): book_name = scrapy.Field() book_star = scrapy.Field() book_pl = scrapy.Field() book_author = scrapy.Field() book_publish = scrapy.Field() book_date = scrapy.Field() book_price = scrapy.Field() 编写Spider访问豆瓣的日本文学标签,将url的值写到start_urls中。接着在Chrome的帮助下，可以看到每本图书是在ul#subject-list &gt; li.subject-item 日本文学 1234567891011121314151617class BookSpider(scrapy.Spider): ... def parse(self, response): sel = Selector(response) book_list = sel.css('#subject_list &gt; ul &gt; li') for book in book_list: item = BookItem() item['book_name'] = book.xpath('div[@class=\"info\"]/h2/a/text()').extract()[0].strip() item['book_star'] = book.xpath(\"div[@class='info']/div[2]/span[@class='rating_nums']/text()\").extract()[ 0].strip() item['book_pl'] = book.xpath(\"div[@class='info']/div[2]/span[@class='pl']/text()\").extract()[0].strip() pub = book.xpath('div[@class=\"info\"]/div[@class=\"pub\"]/text()').extract()[0].strip().split('/') item['book_price'] = pub.pop() item['book_date'] = pub.pop() item['book_publish'] = pub.pop() item['book_author'] = '/'.join(pub) yield item 测试一下代码是否有问题 scrapy crawl book -o items.json 奇怪的发现，items.json内并没有数据，后头看控制台中的DEBUG信息 2017-02-04 16:15:38 [scrapy.core.engine] INFO: Spider opened2017-02-04 16:15:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2017-02-04 16:15:38 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232017-02-04 16:15:39 [scrapy.core.engine] DEBUG: Crawled (403) (referer: None)2017-02-04 16:15:39 [scrapy.core.engine] DEBUG: Crawled (403) (referer: None) 爬取网页时状态码是403。这是因为服务器判断出爬虫程序，拒绝我们访问。我们可以在settings中设定USER_AGENT的值，伪装成浏览器访问页面。 USER_AGENT = “Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)” 再试一次，就发现items.json有值了。但仔细只有第一页的数据，如果我们想要爬取所有的数据，就需要爬完当前页后自动获得下一页的url，以此类推爬完所有数据。所以我们对spider进行改造。12345678910111213141516171819202122...def parse(self, response): sel = Selector(response) book_list = sel.css('#subject_list &gt; ul &gt; li') for book in book_list: item = BookItem() try: item['book_name'] = book.xpath('div[@class=\"info\"]/h2/a/text()').extract()[0].strip() item['book_star'] = book.xpath(\"div[@class='info']/div[2]/span[@class='rating_nums']/text()\").extract()[0].strip() item['book_pl'] = book.xpath(\"div[@class='info']/div[2]/span[@class='pl']/text()\").extract()[0].strip() pub = book.xpath('div[@class=\"info\"]/div[@class=\"pub\"]/text()').extract()[0].strip().split('/') item['book_price'] = pub.pop() item['book_date'] = pub.pop() item['book_publish'] = pub.pop() item['book_author'] = '/'.join(pub) yield item except: pass nextPage = sel.xpath('//div[@id=\"subject_list\"]/div[@class=\"paginator\"]/span[@class=\"next\"]/a/@href').extract()[0].strip() if nextPage: next_url = 'https://book.douban.com'+nextPage yield scrapy.http.Request(next_url,callback=self.parse) 其中scrapy.http.Request会回调parse函数，用try…catch是因为豆瓣图书并不是格式一致的。遇到有问题的数据，就抛弃不用。 突破反爬虫一般来说，如果爬虫速度过快。会导致网站拒绝我们的访问，所以我们需要在settings设置爬虫的间隔时间，并关掉COOKIES DOWNLOAD_DELAY = 2COOKIES_ENABLED = False 或者，我们可以设置不同的浏览器UA或者IP地址来回避网站的屏蔽下面用更改UA来作为例子。在middlewares.py,编写一个随机替换UA的中间件，每个request都会经过middleware。其中process_request，返回None，Scrapy将继续到其他的middleware进行处理。12345678class RandomUserAgent(object): def __init__(self,agents): self.agents = agents @classmethod def from_crawler(cls,crawler): return cls(crawler.settings.getlist('USER_AGENTS')) def process_request(self,request,spider): request.headers.setdefault('User-Agent',random.choice(self.agents)) 接着道settings中设置123456789DOWNLOADER_MIDDLEWARES = &#123;'douban.middlewares.RandomUserAgent': 1,&#125;...USER_AGENTS = [ \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\", \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\", ...] 再次运行程序，显然速度快了不少。 保存到MongoDB接下来我们要将数据保存到数据库做持久化处理(这里用MongoDB举例，保存到其他数据库同理)。这部分处理是写在pipelines中。在此之前我们还要先安装连接数据库的驱动。 pip install pymongo 我们在settings写下配置12345# MONGODB configureMONGODB_SERVER = 'localhost'MONGODB_PORT = 27017MONGODB_DB = 'douban'MONGODB_COLLECTION = \"book\" 1234567891011121314class MongoDBPipeline(object): def __init__(self): connection = MongoClient( host=settings['MONGODB_SERVER'], port=settings['MONGODB_PORT'] ) db = connection[settings['MONGODB_DB']] self.collection = db[settings['MONGODB_COLLECTION']] def process_item(self, item, spider): self.collection.insert(dict(item)) log.msg(\"Book added to MongoDB database!\", level=log.DEBUG, spider=spider) return item 其他将运行项目的时候控制台中输出的DEBUG信息保存到log文件中。只需要在settings中设置 LOG_FILE = “logs/book.log” 项目代码地址：豆瓣图书爬虫","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"},{"name":"Scrapy","slug":"Scrapy","permalink":"http://amoyiki.com/tags/Scrapy/"}]},{"title":"Scrapy学习（二） 入门","date":"2017-01-30T14:12:13.000Z","path":"2017/01/30/Scrapy学习（二）-入门/","text":"快速入门接上篇Scrapy学习（一） 安装，安装后，我们利用一个简单的例子来熟悉如何使用Scrapy创建一个爬虫项目。 创建一个Scrapy项目在已配置好的环境下输入 scrapy startproject dmoz 系统将在当前目录生成一个myproject的项目文件。该文件的目录结构如下123456789dmoz/ # 项目根目录 scrapy.cfg # 项目配置文件 dmoz/ # 项目模块 __init__.py items.py # 项目item文件，有点类似Django中的模型 pipelines.py # 项目pipelines文件，负责数据的操作和存储 settings.py # 项目的设置文件. spiders/ # 项目spider目录，编写的爬虫脚步都放此目录下 __init__.py 接下来我们以dmoz.org为爬取目标。开始变现简单的爬虫项目。 编写items在items.py中编写我们所需的数据的模型123456from scrapy.item import Item, Fieldclass Website(Item): name = Field() description = Field() url = Field() 这个模型用来填充我们爬取的数据 编写Spider在spiders文件下新建爬虫文件。这部分才是业务的核心部分。首先创建一个继承scrapy.spiders.Spider的类并且定义如下三个属性 name 标识spider start_urls 启动爬虫时进行爬取的url列表，默认为空 parse() 每个初始的url下载后的response都会传到该方法内，在这个方法里可以对数据进行处理。 12345678910111213141516171819202122232425262728from scrapy.spiders import Spiderfrom scrapy.selector import Selectorfrom dirbot.items import Websiteclass DmozSpider(Spider): name = \"dmoz\" allowed_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\", ] def parse(self, response): sites = response.css('#site-list-content &gt; div.site-item &gt; div.title-and-desc') items = [] for site in sites: item = Website() item['name'] = site.css( 'a &gt; div.site-title::text').extract_first().strip() item['url'] = site.xpath( 'a/@href').extract_first().strip() item['description'] = site.css( 'div.site-descr::text').extract_first().strip() items.append(item) return items 其中值得注意的是，在parse方法内，我们可以用Selector选择器来提取网站中我们所需的数据。提取的方式有几种。 xpath() 传入xpath表达式获取节点值 css() 传入css表达式获取节点值 re() 传入正则表达式获取节点值 # 此方法本人未测试 运行并保存数据接下来我们运行爬虫，并将爬取的数据存储到json中 scrapy crawl dmoz -o items.json 其他在运行爬虫的过程中，我遇到了如下报错： KeyError: ‘Spider not found: dmoz 这个是因为我的spider类中设置的name的值和我scrapy crawl运行的spider不一致导致的。 具体代码详见：scrapy入门项目","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"},{"name":"Scrapy","slug":"Scrapy","permalink":"http://amoyiki.com/tags/Scrapy/"}]},{"title":"Scrapy学习（一） 安装","date":"2017-01-29T09:10:11.000Z","path":"2017/01/29/Scrapy学习（一）-安装/","text":"在开篇之前，不得不吐槽一下，配置Scrapy是我搞python后配置环境最久的一次了。我赶紧将四小时的配置过程写下来，以免浪费了这些宝贵的踩坑经验。 安装在安装Scrapy之前，要先安装相关的依赖模块，否则无论你是手动pip安装还是用IDE（如Pycharm自动安装都会报错 error: Unable to find vcvarsall.bat 因为pip无法正常的安装一些依赖，所以我们要用wheel来安装。 安装wheel及下载.whl文件 pip install wheel 验证wheel是否正确安装 image 接下来，下载网上人家编译好的.whl文件 http://www.lfd.uci.edu/~gohlke/pythonlibs/ 用Ctrl + F搜索如下文件 lxml-3.7.2-cp35-cp35m-win_amd64.whlpywin32-220.1-cp35-cp35m-win_amd64.whlzope.interface-4.3.3-cp35-cp35m-win_amd64.whlpyOpenSSL-16.2.0-py2.py3-none-any.whlTwisted-16.6.0-cp35-cp35m-win_amd64.whlScrapy-1.3.0-py2.py3-none-any.whl 注： cp后为python版本，我是64位python3.5的版本 最后测试一下安装是否成功 scrapy startproject myproject","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"},{"name":"Scrapy","slug":"Scrapy","permalink":"http://amoyiki.com/tags/Scrapy/"}]},{"title":"[LeetCode]70. Climbing Stairs","date":"2017-01-20T09:28:22.000Z","path":"2017/01/20/LeetCode-70-Climbing-Stairs/","text":"问题描述有n步楼梯，需要n步才能到顶。每次你可以选择爬1步或2步，需要多少步你才能爬到顶。 解题思路这个题目跟斐波那契数列一样。爬到最后一步有两种方式： f(n-1) 爬一步到顶 f(n-2) 爬两步到顶同时爬1步只有一种方式，爬两步有两种方式。 具体代码python123456class Solution(object): def climbStairs(self,n): a,b = 1,1 for i in range(n): a,b = b,a+b return a java123456789101112public class Solution&#123; public int climbStairs(int n)&#123; if(n==0) return 0; int a=1,b=1; for(int i=0;i&lt;n;i++)&#123; int temp = a; a = b; b = a+temp; &#125; return a; &#125;&#125; Climbing Stairs 这题还有一个变种题。题目如下 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 代码 递归12345class Solution: def jumpFloorII(self, number): if number &lt;=0: return -1 if number == 1: return 1 return 2 * self.jumpFloorII(number-1) 非递归123456789class Solution: def jumpFloorII(self, number): # write code here if number &lt;= 0: return -1 if number == 1: return 1 a = 2 for i in range(2,number): a *=2 return a 还有一道斐波那契数列的变种，题目如下 用21的小矩形横、竖去覆盖更大的矩形。用n个21的小矩形无重叠的覆盖一个2n的大矩形。共有多少种方法。如果因为是覆盖一个2n的矩阵，所以当一个21的小矩阵竖放时，大矩形的长度就会变成n-1，剩下的需要覆盖的面积就是2(n-1)如果是横放的话剩下需要覆盖的面积是2*(n-2) ,由此推出f(n) = f(n-1) + f(n-2) 非递归12345678910class Solution: def rectCover(self, number): if number &lt;= 0: return 0 t = [1,2] if number &lt;= 2: return t[number-1] for i in range(2,number): t.append(t[i-1]+t[i-2]) return t[-1] 递归123456789101112public class Solution&#123; public int RectCover(int target)&#123; if(target&lt;=0)&#123; return 0; &#125;else if(target==1)&#123; return 1; &#125;else if(target==2)&#123; return 2; &#125; return RectCover(target-1)+RectCover(target-2); &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"Docker入门","date":"2017-01-20T07:55:15.000Z","path":"2017/01/20/Docker入门/","text":"","tags":[]},{"title":"[LeetCode]153.Find Minimum in Rotated Sorted Array","date":"2017-01-19T09:45:10.000Z","path":"2017/01/19/LeetCode-153-Find-Minimum-in-Rotated-Sorted-Array/","text":"问题描述一个顺序数组，经过一次旋转{1,2,3,4,5} —&gt; {3,4,5,1,2}找出最小元素。 解题思路无论Java还是Python，只要排序过后就可以找出最小元素。时间复杂度O(n)如Python方法min()就能直接得出数组最小值。Java也有Arrays.sort()方法排序。但是如果单纯O(n)的复杂度，明显没有多大的意义。所有这道题应该用二分查找方法来返回最小值。 具体代码python12345678910class Solution(object): def findMin(self,nums): left,right = 0, len(nums) - 1 while left &lt; right: mid = (left + right) / 2 if nums[mid] &lt;= nums[right]: right = mid else: low = mid + 1 return nums[low]","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[转]Java集合小抄","date":"2017-01-18T07:18:39.000Z","path":"2017/01/18/转-Java集合小抄/","text":"集合ListArrayList 以数组实现，节约空间。但是数组容量限制，超出限制会增加50%容量，用System.arraycopy()复制到新的数组。因此最好能给出数组大小的预估值。 默认第一次大小为10。 按下标访问元素，get(下标获取元素)、set(替换下标元素)的性能很高，是数组的基本优势。 按下标插入元素，删除元素，add(i,e)、remove(i)、remove(e),则会用到System.arraycopy()来复制移动受影响的元素,性能会变差。 越是前面的元素，修改的时候移动的元素越多。用add(e)在尾部添加元素、删除最后一个元素不会影响性能。LinkedList 以双向链表实现，链表无容量限制，但是双向链表本身使用了更多空间，每插入一个元素都要构造一个额外的Node对象，也需要额外的指针操作。 按数组下标访问元素，get(i)、set(i,e),需要移动到指定Node节点(i&gt;节点个数时从尾部移动到头部) 插入、删除元素时修改前后节点指针即可 只有在两头add()、addFirst()、addLast()、removeFirst()、removeLast()才能省掉指针的移动。 IteratorIterator 支持从集合中安全删除对象，只需在Iterator上调用remove().ArrayList继承了AbstractList，而AbstractList有定义123456789101112/** * The modCount value that the iterator believes that the backing * List should have. If this expectation is violated, the iterator * has detected concurrent modification. */int expectedModCount = modCount;...final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125;... checkForComodification这个方法是检查modCount和expectedModCount的值是否是相对，如果不相等则抛出ConcurrentModificationException异常。这是因为在多线程中操作容器时，其他线程可能已经改变了容器的内容，所以每次对容器进行操作的时候modCount都会+1。当Iterator遍历检查到modCount变化是会马上抛出异常，这是Java的fail-fast机制。而Iterator的remove方法在操作完后让expectedModCount和modCount在此相等 MapHashMap 以Enty[]数组实现的哈希桶数组，用Key的哈希值取模桶数组的大小可以的到数组的下标 插入元素时，如果两条Key落在同一个桶，称之为哈希冲突或者碰撞 哈希冲突JDK8之前是用的是链表法，用Entry用一个next属性实现多个Entry以单向链表的形式存放。然后遍历链表中的所有元素，逐一比较Key值 在JDK8后，新增链表的阈值为8，链表的元素数量超过阈值时改用红黑树来存储，提高查找速度。 当Entry数量达到数组的75%，数组成倍扩容，重新分配原来的Entry，扩容成本不低。LinkedHashMap 扩展HashMap，每个Entry增加双向链表，非常占内存的数据结构ConcurrentSkipListMap JDK6新增并发优化的SortedMap，以SkipList结构实现，支持CAS无锁算法。 –待整理– 线程方法区和堆一样，是线程共享的。在方法区中，存储每个类的信息（类名，方法信息，字段信息）、静态变量、常量以及编译器编译后的代码。 基本数据类型int -&gt; Integer Integer a = 59;int b = 59;Integer c = Integer.valueOf(59);Integer d = new Integer(59); a 会调用valueOf方法 这个方法返回的是一个Integer对象。下面是它的源码123456public static Integer valueOf(int i) &#123; assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 可以看出当i的值在[-128,127]而且在IntegerCache中存在此对象，就直接返回引用，没有的话创建一个新的对象。显然在这个a之前内存还没有59这个值，所有它创建了一个Integer对象b 是基本类型，存储在栈中c 也用了valueOf这个方法,此时IntegerCache已经存在这个对象，返回引用。d 是一个新的Integer对象在比较int和Integer值时，Integer会调用intValue自动拆箱成int，进行值比较。","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]232. Implement Queue using Stacks","date":"2017-01-11T03:28:14.000Z","path":"2017/01/11/LeetCode-232-Implement-Queue-using-Stacks/","text":"问题描述利用堆栈实现队列功能 解题思路Stack的特点是先进后出，Queue的特点是先进先出。利用两个Stack，一个负责输入一个负责输出.如果输出堆栈为空，将输入堆栈值传给输出堆栈，若两个堆栈都为空，返回空值python中用list模拟堆栈. 具体代码python1234567891011121314151617181920class Queue(object): def __init__(self): self.stackA = [] self.stackB = [] def push(self, x): self.stackA.append(x) def pop(self): self.peek() self.stackB.pop() def peek(self): if self.stackB: return self.stackB[-1] elif not self.stackA: return None else: while self.stackA: self.stackB.append(self.stackA.pop()) return self.stackB[-1] def empty(self): return len(self.stackA)==0 and len(self.stackB)==0 Java1234567891011121314151617181920212223242526public class MyQueue&#123; Stack&lt;Integer&gt; input; Stack&lt;Integer&gt; output; public MyQueue()&#123; input = new Stack&lt;Integer&gt;(); output = new Stack&lt;Integer&gt;(); &#125; public void push(int x)&#123; input.push(x); &#125; public int pop()&#123; peek(); return output.pop(); &#125; public int peek()&#123; if(output.empty())&#123; while(!input.empty())&#123; output.push(input.pop()); &#125; &#125; return output.peek(); &#125; public boolean empty()&#123; return input.empty() &amp;&amp; output.empty(); &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"Nginx学习笔记（二）配置及性能调优","date":"2017-01-06T01:39:42.000Z","path":"2017/01/06/Nginx学习笔记（二）/","text":"如何使用配置文档1.nginx文档结构12345678910111213141516171819202122... #全局块events &#123; #events块 ... &#125;http &#123; #http块 ... #http全局块 server &#123; #server块 ... #server全局块 location [PATTRERN] &#123; #location块 ... &#125; location [PATTRERN] &#123; ... &#125; &#125; ...&#125; 全局块：配置会影响nginx全局指令 events块：配置事件驱动模型和最大连接数 http块：可以嵌套多个server，配置代理，缓存，日志 server块： location块： –施工中– 性能调优 –施工中–","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]105.Construct Binary Tree from Preorder and Inorder Traversal","date":"2017-01-05T09:52:05.000Z","path":"2017/01/05/LeetCode-105-Construct-Binary-Tree-from-Preorder-and-Inorder-Traversal/","text":"问题描述给定树的前序遍历和中序遍历，构建出这个二叉树 解题思路相对于Python的数组切割,Java取下标值来获取每次递归所需的值区间。所以需要一个辅助函数来负责递归。前序遍历中第一个元素pre[0]一定是root节点。假设该节点的元素在中序中是在in[5],那么中序中的in[0] ~ in[4]就是左子树，in[6] ~ end 就是右子树。第二次递归，左子树从pre[1]开始(即pre[0]的下移一位)。中序数组从0~4右子树从pre[0+5-0+1]开始,中序数组从6~end第三次… 具体代码python12345678910111213class Solution(object): def buildTree(self, preorder, inorder): if not preorder or not inorder: return None rootValue = preorder.pop(0) root = TreeNode(rootValue) inorderIndex = inorder.index(rootValue) root.left = self.buildTree(preorder, inorder[:inorderIndex]) root.right = self.buildTree(preorder, inorder[inorderIndex+1:]) return root Java123456789101112131415161718192021222324public class Soultion&#123; public TreeNode buildTree(int[]preorder, int[] inorder)&#123; return helper(0,0,inorder.length-1,preorder,inorder); &#125; public TreeNode helper(int preStart, int inStart, int inEnd, int[] pre,int[] in)&#123; if(preStart &gt; pre.length - 1 || inStart &gt; inEnd)&#123; return null; &#125; TreeNode root = new TreeNode(pre[preStart]); int inIndex = 0; // root节点在中序数组中的下标位置 for(int i == inStart; i&lt;= inEnd; i++)&#123; if(in[i]==root.val)&#123; inIndex = i; &#125; &#125; /* 左子树：前序数组从当前root下标的下一位开始，中序数组从开头开始到当前root下标的前一位结束 右子树：前序数组从当前root下标的下一位，中序数组从root下标下一位到结束 */ root.left = helper(preStart + 1, inStart, inIndex - 1, pre, in); root.right = helper(preStart + inIndex - inStart + 1, inIndex + 1, inEnd , pr, in); return root; &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]74. Search a 2D Matrix","date":"2016-12-21T02:42:05.000Z","path":"2016/12/21/LeetCode-74-Search-a-2D-Matrix/","text":"问题描述在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 解题思路一开始打算用二分查找的方式，将二维数组划分成两部分，递归判断。实际写出来的时候就发现，二维数组并不好去判断值。后来在网上看到一个更好的方法。 将二维数组转化成二维平面，将target值从第一行的最后一个值开始查找，小于就往左边移动，大于就往下面移动。 1 3 5 7 10 11 16 20 23 30 34 50 具体代码python12345678910111213141516class Solution(object): def searchMatrix(self, matrix, target): if matrix is None or len(matrix)==0: return False row = len(matrix) col = len(matrix[0]) i=col-1 j=0 while(i&gt;=0 and j&lt;row): if matrix[j][i] == target: return True elif matrix[j][i] &gt; target: i = i - 1 elif matrix[j][i] &lt; target: j = j + 1 return False java123456789101112131415161718public class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; if(matrix==null || matrix.length==0 || matrix[0].length==0) return false; int i = 0; int j = matrix[0].length - 1; while (i &lt;= matrix.length - 1 &amp;&amp; j &gt;= 0) &#123; if (target == matrix[i][j]) &#123; return true; &#125; else if (target &lt; matrix[i][j]) &#123; j--; &#125; else if (target &gt; matrix[i][j]) &#123; i++; &#125; &#125; return false; &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"离线安装VMWare Tools","date":"2016-12-19T08:26:33.000Z","path":"2016/12/19/离线安装WMWare-Tools/","text":"VMWare Tools 解压tools文件win+R打开运行窗口,解压exe文件 ..\\tools-windows-*.exe /e .\\解压成iso文件msiexec /a “..\\tools-windows.msi” /qb TARGETDIR=”.\\” 虚拟机加载iso即可安装VMWare Tools","tags":[{"name":"WMWare","slug":"WMWare","permalink":"http://amoyiki.com/tags/WMWare/"}]},{"title":"Python协程","date":"2016-12-16T03:04:21.000Z","path":"2016/12/16/Python协程/","text":"– 施工中 – Because generator-iterators begin execution at the top of the generator’s function body, there is no yield expression to receive a value when the generator has just been created. Therefore, calling send() with a non-None argument is prohibited when the generator iterator has just started, and a TypeError is raised if this occurs (presumably due to a logic error of some kind). Thus, before you can communicate with a coroutine you must first call next() or send(None) to advance its execution to the first yield expression. 因为生成器在执行函数时，生成器刚被创建，没有接收到yield表达式的值隐藏当生成器启动时禁止进行无参send()方法 生产者消费者123456789101112131415161718def consumer(): # 定义一个消费者，由于有yeild关键字，此方法是个生成器 print '[Consumer] Init Consumer ...' r = 'init ok' # 初始化返回结果，在启动消费者的时候，返回给生成者 while True: n = yield r # 消费者通过yield接收生产者的消息，并返回其结果 print '[Consumer] consume n = %s, r=%s'%(n,r) r = 'consume %s OK' % n # 消费者消费结果，下个循环返回给生成者def produce(c): # 定义一个生产者，c为消费者生成器 print '[Producer] Init Producer ...' r = c.send(None) # 启动消费者生成器，同时返回第一次结果 n = 0 while n &lt; 5: n += 1 print '[Producer] While, Producing %s ...'%n r = c.send(n) # 向消费者发送消息并准备接收结果。此时会切换到消费者执行 print '[Producer] Consumer return: %s'%r c.close() print '[Producer] Close Producer ....'","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"git技巧--gitignore配置","date":"2016-12-15T05:54:08.000Z","path":"2016/12/15/git技巧-gitignore配置/","text":".gitignore 文件配置.gitignore 配置无法解决问题时如果.gitignore配置完后，仍无法屏蔽掉特殊文件的情况。我们可以手动修改项目路径下的.git/info/exclude文件。例如： 在Pycharm下进行Python开发时会生成临时文件__pycache__/,只需将这个文件名写到exclude文件内即可。","tags":[{"name":"git","slug":"git","permalink":"http://amoyiki.com/tags/git/"}]},{"title":"编写一个Chrome extension--网页二维码生成","date":"2016-12-13T14:11:33.000Z","path":"2016/12/13/编写一个Chrome-extension-网页二维码生成/","text":"很早之前就想过要用Chrome扩展开发一些实用的，或者有意思扩展。今天在看了segmentfault的技术周刊后，决定先按照别人写过的东西去抄一遍模仿的做一遍。 本篇文章是看了从小目标开始，编写一个简洁的二维码chrome扩展模仿的。这篇文章写得很详细。我主要写写自己模仿过程中的一些问题。 Chrome extension基础编写Chrome 扩展之前我们需要大致的了解一下Google提供的开发文档。鉴于我可怜的英文水平，我推荐花几分钟看一下下面的文档： 360翻译的官方API文档 Chrome扩展及应用开发 ←这本书不仅介绍API用法，还提供了好多实例。 编写过程有了上面几分钟的基础后，我们可以开始正式编写代码了。创建一个文件夹，将扩展所创建的文件都放在里面，方便完成后打包。 首先编写manifest.json这是所有扩展的入口文件。看到后缀我们就知道这文件的语法结构必须符合json的写法。Chrome 扩展必须包含的属性有name、version、manifest_version。其他可选属性包括：background、permissions、browser_action、page_action、options_page、content_scripts等等。1234567891011121314151617&#123; //目前Chrome版本为2 \"manifest_version\": 2, //扩展名称 \"name\": \"QRcode\", //扩展版本，可自定义 \"version\": \"1.0\", //扩展描述，显示在扩展程序中 \"description\": \"简洁的二维码生成器\", //显示在扩展程序中的图标 \"icons\": &#123; \"16\": \"images/icon16.png\", \"128\": \"images/icon128.png\" &#125;, //权限声明 \"permissions\":[\"tabs\"]&#125; 接下来就要编写扩展弹出页面popup.html文件popup页面在被用户点击时初始化，关闭后就会销毁。所以该页面更多的是用来展示结果的。数据处理则需要background这个属性来声明，这里暂时没用到就不多说了。需要注意的是，应该用css指定popup页面大小。另外，Google不允许HTML和JavaScript混写在同一个文件内。所有我们把相应的JS提出来，在HTML中添加外部引用。1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style type=\"text/css\"&gt; .box &#123; height: 200px; width: 200px; background: #EEE; &#125; .box .title&#123; text-align: center; margin-bottom: 10px; &#125;&lt;/style&gt;&lt;body&gt;&lt;div class=\"box\"&gt; &lt;div class=\"title\"&gt;扫描二维码浏览本页面&lt;/div&gt; &lt;center&gt; &lt;div class=\"qrcode\" id=\"qrcode\"&gt;&lt;/div&gt; &lt;/center&gt;&lt;/div&gt;&lt;script src=\"js/qrcode.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"js/popup.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 编写相应的popup.js文件chrome.tabs这个API可以与浏览器的标签页系统进行交互。具体API说明参考标签–扩展开发文档通过获取到的标签页url传给QRCode。通过QRCode.js生成二维码。123456789101112131415onload=function()&#123; chrome.tabs.getSelected(function(tab)&#123; //QRCode(元素id,相关配置文件) var qrcode = new QRCode(\"qrcode\", &#123; text: tab.url, width: 160, height: 160, colorDark : '#000000', colorLight : '#ffffff', // QRCode的容错级别 correctLevel : QRCode.CorrectLevel.H &#125;); console.log(qrcode); &#125;);&#125; 到目前为止，一个简单的QRCode生成器边完成了。 加载自定义插件 QRCode 后续如果想让二维码中间位置显示自定义图片(如上图)，那么只需要在popup页面自定义一段CSS即可。 修改popup.html文件12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style type=\"text/css\"&gt; .box &#123; height: 200px; width: 200px; background: #EEE; position: relative; &#125; .box .title&#123; text-align: center; margin-bottom: 10px; &#125; .box .qrcode&#123; width: 100%; height: 100%; position: absolute; &#125; .box .logo &#123; top: 80px; left: 80px; width: 40px; height: 40px; position: absolute; &#125;&lt;/style&gt;&lt;body&gt;&lt;div class=\"box\"&gt; &lt;div class=\"title\"&gt;扫描二维码浏览本页面&lt;/div&gt; &lt;center&gt; &lt;div class=\"qrcode\" id=\"qrcode\"&gt;&lt;/div&gt; &lt;div class='logo'&gt; &lt;img src=\"http://amoyiki.github.io/images/avatar.jpg\" width=\"40\" height=\"40\"/&gt; &lt;/div&gt; &lt;/center&gt;&lt;/div&gt;&lt;script src=\"js/qrcode.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"js/popup.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 动态获取网页icon如果想让icon图标随着每个网站不同的icon进行变动的话，就只要利用tab的favIconUrl属性就能得到标签页面的图标url地址,改动如下123456789101112131415161718onload=function()&#123; chrome.tabs.getSelected(function(tab)&#123; //QRCode(元素id,相关配置文件) var qrcode = new QRCode(\"qrcode\", &#123; text: tab.url, width: 160, height: 160, colorDark : '#000000', colorLight : '#ffffff', // QRCode的容错级别 correctLevel : QRCode.CorrectLevel.H &#125;); if (tab.favIconUrl) &#123;//tab有图标的情况下动态赋值 var img = document.getElementsByTagName(\"img\")[1].src = tab.favIconUrl; &#125; console.log(img); &#125;);&#125; 详细代码可以查看github源码地址","tags":[]},{"title":"[LeetCode]290. Word Pattern","date":"2016-12-12T08:48:27.000Z","path":"2016/12/12/LeetCode-290-Word-Pattern/","text":"问题描述给定一个模式pattern和一个字符串str，判断str是否满足相同的pattern。例如：pattern = “abba”, str = “dog cat cat dog” 则返回 true. 解题思路判断pattern，str每个元素出现的下标，求得的数组相等即返回true 具体代码python12345class Solution(object): def wordPattern(self, pattern, str): p = pattern s = str.split() return map(p.find,p) == map(s.index,s) Java12345678910111213141516171819public class Solution &#123; public boolean wordPattern(String pattern, String str) &#123; String[] words = str.split(\" \"); if(pattern.length() != words.length) return false; Set&lt;String&gt; set = new HashSet&lt;String&gt;(); Map&lt;Character,String&gt; map = new HashMap&lt;Character,String&gt;(); for(int i=0;i&lt;words.length;i++)&#123; char p = pattern.charAt(i); if (map.containsKey(p))&#123; if(!map.get(p).equals(words[i])) return false; &#125;else &#123; if (set.contains(words[i])) return false; map.put(p,words[i]); set.add(words[i]); &#125; &#125; return true; &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]303. Range Sum Query - Immutable","date":"2016-12-07T06:34:39.000Z","path":"2016/12/07/LeetCode-303-Range-Sum-Query-Immutable/","text":"问题描述给定一个数字数组，求下标在i和j(i ≤ j)之间的元素和 解题思路利用一个辅助数组sums[x+1]来存储当前位置与之前元素的累加和 具体代码python1234567891011class NumArray(object): def __init__(self,nums): size = len(nums) # 辅助函数sums，计算每个位置与之前的数字累积和 self.sums = [0] * (size + 1) for x in range(size): # 当前(x+1)位置元素累积和 = 前一位(累积和)+当前元素 self.sums[x + 1] += self.sums[x] + nums[x] def sumRange(self,i,j): return self.sums[j+1] - self.sums[i] Java1234567891011121314public class NumArray &#123; public int[] sums; public NumArray(int[] nums) &#123; int size = nums.length; sums = new int[size+1]; for(int i=0;i&lt;size;i++)&#123; sums[i+1] = sums[i] + nums[i]; &#125; &#125; public int sumRange(int i, int j) &#123; return sums[j+1] - sums[i]; &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"HashMap,Hashtable,ConcurrentHashMap,SynchronizedMap的原理与区别","date":"2016-12-07T02:28:18.000Z","path":"2016/12/07/了解HashMap/","text":"HashMapHashMap的碰撞处理HashMap通过hashCode()方法来确定元素存储的bucketIndex位置，不同的Key有概率hash是相同的。两个不同Key的hash值相同时，HashMap通过单链表方式，将新元素加入链表表头，通过next指向原有元素。 在JDK1.8版本中，只要bucket中的链表长度超过阈值（8）时，会将链表转化为红黑树在JDK1.7中HashMap的put方法源码如下：1234567891011121314151617181920212223242526public V put(K key, V value) &#123; ... //处理Key为null if (key == null) return putForNullKey(value); //得到key的hash码 int hash = hash(key); //由hash码获取bucketIndex下标 int i = indexFor(hash, table.length); //取出bucketIndex上元素，形成单链表 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //hash码相同时且对象相同时 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; //替换旧值 V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; //key不存在，加入新元素 modCount++; addEntry(hash, key, value, i); return null; &#125; 为什么HashMap线程不安全 并发时，多线程同时操作使用put方法添加元素，如果发生碰撞，可能会导致两个值添加到同一位置，致使最终有一个值被覆盖 多线程使用HashMap进行扩容时，可能会形成循环链路，详情可以看看Java HashMap的死循环","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"Java垃圾回收","date":"2016-12-06T08:32:20.000Z","path":"2016/12/06/Java垃圾回收/","text":"GC 如何判断对象可以被回收引用计数算法早期Java使用的算法，现已不用。 可达性分析算法以GC Root 作为起始点，向下搜索。走过的路径成为引用链。当一个对象到GC Root没有任何引用链相连时，证明对象不可用。Java的GC Root对象包括： 虚拟机栈（栈帧中的本地变量表中引用对象） 方法区中类静态属性引用对象 方法区常量引用对象 本地方法JNI引用对象","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]463. Island Perimeter","date":"2016-11-23T02:01:55.000Z","path":"2016/11/23/LeetCode-463-Island-Perimeter/","text":"问题描述给定一个二维地图，1表示陆地，0表示水域。每一个陆地是边长为1的正方形。求岛屿的周长。 解题思路每个格子周长为4，两个格子相邻时周长-2 具体代码python1234567891011121314class Solution(object): def islandPerimeter(self, grid): ans = 0 h = len(grid) w = len(grid[0]) if h else 0 for x in range(h): for y in range(w): if grid[x][y] == 1: ans += 4 if x &gt; 0 and grid[x-1][y]: ans -= 2 if y &gt; 0 and grid[x][y-1]: ans -= 2 return ans Java1234567891011121314151617public class Solution &#123; public int islandPerimeter(int[][] grid) &#123; int ans = 0; int h = g.length; int w = g[0].length; for(int i=0;i&lt;h;i++)&#123; for (int j=0;j&lt;w;j++)&#123; if (g[i][j] == 1)&#123; ans += 4; if(i &gt; 0 &amp;&amp; g[i-1][j] == 1) ans -= 2; if(j &gt; 0 &amp;&amp; g[i][j-1] == 1) ans -= 2; &#125; &#125; &#125; return ans &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]217. Contains Duplicate","date":"2016-11-10T06:44:59.000Z","path":"2016/11/10/LeetCode-217-Contains-Duplicate/","text":"问题描述给定一个整数数组，判断是否包含重复元素，是返回true。若都是唯一返回false 解题思路利用set这种数据结构的特点，只要set后的数据长度不等于原来的数据长度的话，就证明有重复元素，否则证明没有重复元素。 具体代码python123class Solution(object): def containsDuplicate(self, nums): return len(nums) != len(set(nums)) Java12345678910public class Solution &#123; public boolean containsDuplicate(int[] nums) &#123; Set s = new HashSet(); for(int n:nums)&#123; s.add(n); &#125; if(nums.length != s.size()) return true; return false; &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]453.Minimum Moves to Equal Array Elements","date":"2016-11-09T08:21:57.000Z","path":"2016/11/09/LeetCode-453-Minimum-Moves-to-Equal-Array-Elements/","text":"问题描述给定一个长度为n的非空数字数组，每次对n-1个加1。求所有元素值相等，需要几次操作。 解题思路操作次数 = 数组总和 - 数组中最小的数*数组长度 具体代码python123class Solution(object): def minMoves(self, nums): return sum(nums) - min(nums)*len(nums) java1234567891011public class Solution &#123; public int minMoves(int[] nums) &#123; int min = nums[0]; int sum = 0; for(int i: nums)&#123; min = Math.min(min,i); sum +=i; &#125; return sum - min*nums.length; &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"[LeetCode]409.Longest Palindrome","date":"2016-11-09T02:50:21.000Z","path":"2016/11/09/LeetCode-409-Longest-Palindrome/","text":"问题描述求一串字符串最大的回文子字符串长度注意：1.大小写敏感.2.默认字符串全大写或全小写 解题思路偶数字符个数累加；奇数字符个数先减一再累加，同时计算奇数个数。最后如果奇数个数大于0，累加结果再加1。 具体代码python12345678910class Solution(object): def longestPalindrome(self, s): ans = odd = 0 count = collections.Counter(s) for i in count: ans += count[i] if count[i] % 2 == 1: ans -= 1 odd += 1 return ans + (odd &gt; 0) Java1234567891011121314class Soultion &#123; public int longestPalindrome(String s) &#123; int len = 0; boolean[] map = new boolean[128]; for(char c : s.toCharArray())&#123; map[c] = !map[c];//将有字符的位置由false变成true //如果该位置为false的话证明有偶数个数存在 if(!map[c]) len += 2; &#125; //如果字符串长度大于已累计长度，添加一个元素放在回文字符串中间 if (len &lt; s.length())len++; return len; &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"数据结构--Java（2）实现链表","date":"2016-11-07T08:34:03.000Z","path":"2016/11/07/数据结构-Java（2）实现链表/","text":"单向链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116/** * Created by Administrator on 2016-11-07. */public class LinkedList &#123; private class Node&#123; private Node next; private Object obj; public Node(Object obj)&#123; this.obj = obj; &#125; &#125; public Node first; public int pos = 0; public LinkedList()&#123; this.first = null; &#125; public void addFirst(Object obj)&#123; Node node = new Node(obj); node.next = this.first; this.first = node; &#125; public Object delFirst() throws Exception &#123; if(isEmpty())&#123; throw new Exception(\"This LinkedList is empty!\"); &#125; Node temp = this.first; this.first = temp.next; return temp.obj; &#125; public void add(int index, Object obj) throws Exception &#123; if (isEmpty())&#123; throw new Exception(\"This LinkedList is empty!\"); &#125; Node node = new Node(obj); Node cur = first; Node pre = first; while (index != pos)&#123; pre = first; cur = first.next; pos++; &#125; node.next = cur; pre.next = node; pos = 0; &#125; public void remove(Object obj) throws Exception &#123; if (isEmpty())&#123; throw new Exception(\"This LinkedList is empty!\"); &#125; if (first.obj.equals(obj))&#123; this.first = this.first.next; &#125;else&#123; Node pre = this.first; Node cur = this.first.next; while (cur != null)&#123; if (cur.obj.equals(obj))&#123; pre.next = cur.next; break; &#125; pre = cur; cur = cur.next; &#125; if (cur == null)&#123; throw new Exception(\"Not Found\"); &#125; &#125; &#125; public Node find(Object obj) throws Exception &#123; if (isEmpty())&#123; throw new Exception(\"This LinkedList is empty!\"); &#125; Node cur = first; while(cur != null)&#123; if (cur.obj.equals(obj))&#123; return cur; &#125; cur = cur.next; &#125; return null; &#125; public boolean isEmpty()&#123; return (first == null); &#125; public void display()&#123; if(first == null) System.out.println(\"empty\"); Node cur = first; while(cur != null)&#123; System.out.print(cur.obj.toString() + \" -&gt; \"); cur = cur.next; &#125; System.out.print(\"\\n\"); &#125; public static void main(String[] args) throws Exception &#123; LinkedList ll = new LinkedList(); ll.addFirst(4); ll.addFirst(3); ll.addFirst(2); ll.addFirst(1); ll.display(); ll.delFirst(); ll.display(); ll.remove(3); ll.display(); System.out.println(ll.find(1)); System.out.println(ll.find(4).obj); ll.add(1,5); ll.display(); &#125;&#125; out 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt;2 -&gt; 3 -&gt; 4 -&gt;2 -&gt; 4 -&gt;null42 -&gt; 5 -&gt; 4 -&gt;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"数据结构--Java（1） 实现Stack","date":"2016-10-31T07:59:38.000Z","path":"2016/10/31/数据结构-Java（1）-实现Stack/","text":"利用数组实现Stack 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112public class ArrayStack&lt;T&gt; implements StackADT&lt;T&gt; &#123; private final int DEFAULT_SIZE=2;//默认大小 private int capacity;//栈容量 private int size;//栈大小 private int top; private Object[] array; public ArrayStack()&#123; this.capacity = DEFAULT_SIZE; this.array = new Object[this.capacity]; this.size = 0; System.out.println(top); &#125; public ArrayStack(int capacity)&#123; this.capacity = capacity; this.array = new Object[this.capacity]; this.size = 0; &#125; @Override public void clear() &#123; Arrays.fill(this.array,null); this.size = 0; this.top = 0; this.capacity = DEFAULT_SIZE; this.array = new Object[this.capacity]; &#125; @Override public boolean isEmpty() &#123; return this.size == 0; &#125; @Override public T peek() &#123; if (isEmpty())&#123; return null; &#125; return (T) this.array[this.top-1]; &#125; @Override public T pop() &#123; T v = (T) this.array[top-1]; array[this.top-1] = null; this.top = this.top - 1; this.size--; return v; &#125; @Override public void push(T v) &#123; if (this.size&lt;this.capacity)&#123; this.array[top] = v; this.size++; this.top++; &#125;else &#123; addStackCap(); push(v); &#125; &#125; private void addStackCap() &#123;//扩容 this.capacity = this.capacity+DEFAULT_SIZE; Object[] newArray = new Object[this.capacity]; System.arraycopy(this.array, 0, newArray, 0,this.array.length); Arrays.fill(array, null);//原来的数组置空 this.array = newArray; &#125; @Override public int size() &#123; return this.size; &#125; /** * 测试栈 * @param args */ public static void main(String[] args) &#123; ArrayStack&lt;Integer&gt; a = new ArrayStack&lt;Integer&gt;(); a.push(3); a.push(5); a.push(2); a.push(1); a.push(6); System.out.println(\"栈大小:\"+a.size()); System.out.println(\"栈容量:\"+a.capacity); System.out.println(\"栈顶元素:\"+a.peek()); while (!a.isEmpty())&#123; System.out.println(a.pop()); &#125; System.out.println(\"栈大小:\"+a.size()); System.out.println(\"栈容量:\"+a.capacity); System.out.println(\"栈顶元素:\"+a.peek()); System.out.println(\"************\"); a.clear(); System.out.println(\"栈大小:\"+a.size()); System.out.println(\"栈容量:\"+a.capacity); &#125;&#125;interface StackADT&lt;T&gt; &#123; public void clear(); public boolean isEmpty(); public T peek(); public T pop(); public void push(T v); public int size();&#125; 利用LinkedList实现Stack1234567891011121314151617181920212223242526272829303132public class Stack&lt;T&gt; &#123; private LinkedList&lt;T&gt; storage = new LinkedList&lt;T&gt;(); /** 入栈 **/ public void push(T v)&#123; storage.addFirst(v); &#125; /** 出栈 **/ public T pop()&#123; if(isEmpty()) return null; return storage.removeFirst(); &#125; /** 栈为空 **/ public boolean isEmpty() &#123; return storage.isEmpty(); &#125; public String toString()&#123; return storage.toString(); &#125; public void clear()&#123; storage.clear(); &#125; public static void main(String[] args) &#123; Stack stack = new Stack&lt;String&gt;(); stack.push(\"a\"); stack.push(\"b\"); stack.push(\"c\"); System.out.println(stack.toString()); Object obj = stack.pop(); System.out.println(obj+\"------\"+stack.toString()); obj = stack.pop(); obj = stack.pop(); System.out.println(obj); &#125;&#125;","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"Nginx学习笔记（一）安装Nginx","date":"2016-10-16T14:42:14.000Z","path":"2016/10/16/Nginx学习笔记（一）/","text":"在Linux下安装Nginx 在Nginx官网上下载压缩包。 解压后进行安装 在./configure的时候会报错./configure: error: the HTTP rewrite module requires the PCRE library. 这时候我们需要在Linux上安装PCRE库 sudo apt-get update sudo apt-get install libpcre3 libpcre3-dev 再次编译，发现又报了缺少zlib library，我们再次照葫芦画瓢安装zlib sudo apt-get install zlib1g-dev 再次编译，发现Nginx安装成功！ Nginx的启动，关闭命令 Nginx 启动命令 1/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf Nginx 关闭命令 1kill -QUIT PID 将Nginx写成服务运行如果每次都去执行/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf是非常麻烦的事。所以我们将Nginx的相关操作写成Bash脚本，就能像windows服务一样简单的几个命令就能完成任务。首先我们在网上Copy一份&gt;&gt;Ngnix脚本然后在Linux执行命令 12$&gt; sudo wget http://github.com/amoyiki/Blog/raw/master/Document/nginx -O /etc/init.d/nginx$&gt; sudo chmod +x /etc/init.d/nginx 现在我们就可以用简短的命令启动服务了 Usage: /etc/init.d/nginx {start|stop|restart|force-reload|reload|status|configtest|quietupgrade|terminate|destroy}","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"Python一些小技巧","date":"2016-10-10T07:31:11.000Z","path":"2016/10/10/Python一些小技巧/","text":"列表排序 列表和字典的混合排序 12persons = [&#123;'name':'zhang3','age':15&#125;,&#123;'name':'li4','age':12&#125;]persons.sort(lambda a,b:a['age']-b['age']) # 按照年龄排序 两个列表的差异 交集 123a = [1,2,3]b = [2,3,4]value = [v for v in a if v in b] 差集 12...value = [v for v in a if not v in b] 日期datetime 获得今天时间凌晨，格式：’2016-11-02 00:00:00’ 1now = datetime.datetime.today().replace(hour=0,minute=0,second=0,microsecond=0) 时间相加减 12d1 = datetime.datetime.now()d2 = d1 + datetime.timedelta() 其中timedelta方法可以以下几个日期参数进行修改 days=0,seconds=0,microseconds=0,milliseconds=0,minutes=0,hours=0,weeks=0","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":" 编写简单的ORM","date":"2016-08-24T01:51:46.000Z","path":"2016/08/24/编写简单的ORM/","text":"最近在跟着廖雪峰的python3教程，把学习的过程记录下来。日后好反复翻阅 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Field(object): def __init__(self, name, colum_type): self.name = name self.column_type = colum_type def __str__(self): return '&lt;%s : %s&gt;' % (self.__class__.__name__, self.name)class StringField(Field): def __init__(self, name): super(StringField, self).__init__(name, 'varchar(100)')class IntegerField(Field): def __init__(self, name): super(IntegerField, self).__init__(name, 'bigint')class ModelMetaclass(type): def __new__(cls, name, bases, attrs): if name == 'Model': return type.__new__(cls, name, bases, attrs) print(\"Found model: %s\" % name) mappings = dict() for k, v in attrs.items(): if isinstance(v, Field): print(\"Found mapping: %s ==&gt; %s\" % (k, v)) mappings[k] = v for k in mappings.keys(): attrs.pop(k) attrs['__mappings__'] = mappings # 保存属性和映射关系 attrs['__table__'] = name # 表明与类名一致 return type.__new__(cls, name, bases, attrs)class Model(dict, metaclass=ModelMetaclass): def __init__(self, **kw): super(Model, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r\"'Model' object has no attribute '%s' \" % key) def __setattr__(self, key, value): self[key] = value def save(self): fields = [] params = [] args = [] for k, v in self.__mappings__.items(): fields.append(v.name) params.append(\"?\") args.append(getattr(self, k, None)) print('='.join(['name', 'iki'])) sql = \"INSERT INTO %s (%s) VALUES (%s)\" % (self.__table__, ','.join(fields), ','.join(params)) print('SQL: %s' % sql) print('ARGS: %s' % str(args)) def update(self): pass 12345678910111213141516171819class User(Model): # 定义类的属性到列的映射： id = IntegerField('id') name = StringField('username') email = StringField('email') password = StringField('password')u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd')u.save()[OUTPUT]Found model: UserFound mapping: id ==&gt; &lt;IntegerField : id&gt;Found mapping: name ==&gt; &lt;StringField : username&gt;Found mapping: email ==&gt; &lt;StringField : email&gt;Found mapping: password ==&gt; &lt;StringField : password&gt;SQL: INSERT INTO User (username,id,email,password) VALUES (?,?,?,?)ARGS: ['Michael', 12345, 'test@orm.org', 'my-pwd'] 本段代码是照着廖雪峰python3教程敲得，后续会补充一些理解和扩展该代码","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"SQL小技巧","date":"2016-08-18T03:03:00.000Z","path":"2016/08/18/SQL小技巧/","text":"重置表自增长ID值有时候我们需要清空一张表的数据，又不想删除增长表。除了删除数据外，还需要将自增长ID重置为0。相关的SQl操作如下 方法一 1truncate table tb 但是truncate方法不能清空含有Foreign Key约束的表，这个时候就需要用到方法二了。 方法二1DBCC CHECKIDENT(TB,RESEED,0) 循环插入10万条测试数据12345678declare @i intset @i=1while @i&lt;100000begin insert into test(title,date_time) values('test'+cast(@i as nvarchar(10)),getdate()) set @i=@i+1endgo 注意: cast()函数是强制类型转换与convert()用法一致 declare 用于声明变量","tags":[]},{"title":"零碎的小技巧","date":"2016-08-05T01:47:18.000Z","path":"2016/08/05/零碎的小技巧/","text":"如何科学上网作为一名折腾的人，一名苦逼搬砖程序员，最不能缺少的就是和世界接轨。如果不去了解国外的技术发展，就没有办法提升自身的技术水平。所以，科学上网是一个程序员应该有的基础能力。这里简单的介绍一下目前我使用的科学上网工具。 vpn代理这个是最常见，最简单的方法。VPN软件也是多的数不胜数。目前我有在使用的是有两款，都是免费的（说到底还是因为穷）。一款是Lantern，另外一款是赛风。 更改Host这个方法只能浏览一些特定的网站，而且网速也是时好时坏。但是，对于只简单的想使用Google相关网站，还是很好用的。因为Host的IP经常会ban掉，所以需要经常更新一下。更新Host地址 Chrome Google 首页及默认搜索设置在成功的科学上网后，如果用的是Chrome的话，还可以设置下图操作，利用Https防止链接被和谐。 image","tags":[]},{"title":"Vim使用技巧","date":"2016-05-19T00:41:57.000Z","path":"2016/05/19/vim使用技巧/","text":"最近在学习Linux操作系统，顺带的把Vim也稍微的学了一下。下面记录一些常用的Vim操作命令。 Vim移动命令 k,j,h,l 分别是移动光标：上下左右 Ctrl + b / Ctrl + f 分别是上翻一页，下翻一页 gg 移动到文档首 G 移动到文档末","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"},{"name":"vim","slug":"vim","permalink":"http://amoyiki.com/tags/vim/"}]},{"title":"SQL 中Group by 的使用","date":"2016-05-11T02:59:47.000Z","path":"2016/05/11/SQL 中Group by 的使用/","text":"在多表查询中使用Group by 有三张表，一张是部门表-dept，一张是餐厅表-dinner，还有一张是员工消费明细表-cost(含餐厅、部门外键)现在需要一张报表，需求如下: 部门名称 餐厅名称 消费总次数 消费总金额 部门1 餐厅1 2000 20000 部门1 餐厅2 2000 20000 部门2 餐厅1 2000 20000 12345select dept.deptname,dinner.name,cost1.countsum,cost2.money from cost as c left join (select dining_id,dept_id,count(*) as countsum from cost group by dining_id,dept_id) as c1 on c1.dining_id = c.dining_id and c1.dept_id=c.dept_idleft join (select dining_id,dept_id,sum(money) as money from cost group by dining_id,dept_id) as c2 on c2.dining_id = c.dining_id and c2.dept_id=ic.dept_idleft join dinner as d on d.id= c.dining_idleft join dept as dept on dept.deptid= c.dept_id ; 注意： 当使用group by时，select 指定的字段要么作为分组分组依据，写在group by 后边，要吗已经被包含在聚合函数中。如果不是这两种情况的话会跳出错误： 选择列表中的列 ‘xxx’ 无效，因为该列没有包含在聚合函数或 GROUP BY 子句中。","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"Linux注意事项","date":"2016-05-10T03:41:57.000Z","path":"2016/05/10/Linux注意事项/","text":"下载、解压、执行 make make install wget 等命令需要在命令前加上sudo(PS：最好所有命令都加上) 解压与压缩命令 解压命令： tar -zxvf *.tar.gz tar -xvf *.tar unrar e *.rar unzip *.zip 压缩命令： tar -cf all.tar *.jpg # -c新的包，f文件名 tar -rf all.tar *.gif # -r新增加 列出all中所有文件 tar -tf all.tar 切换命令/图形界面 切换成命令界面（暂时） Ctrl+Alt+空格 Ctrl+Alt+F1~F6 切换成命令模式（永久） $&gt;echo “false” | sudo tee /etc/X11/default-display-manager 然后重启Ubuntu 切换成图形界面（暂时） startx 切换成图形界面（永久） $&gt;echo “/usr/sbin/gdm” | sudo tee /etc/X11/default-display-manager Linux 软件安装步骤 加载配置 sudo ./configure 注意： 在此命令后面添加--prefix=...可以指定安装路径，例如sudo ./configure --prefix=/usr/local/python 编译 sudo make 安装 sudo make install Linux 清屏命令 $ clear 保留历史记录，将页面下翻一页而已 $ reset 真正意义上的清空界面 Linux 更改计算机名 $ sudo vim /etc/hostname将第一行改为你想要的名字 Ubuntu apt-get update失败 E: Could not get lock /var/lib/apt/lists/lockE: Could not get lock /var/lib/dpkg/lock将这两个文件删除即可执行update命令 中断命令执行 有时候命令执行到一半发现执行错误，或者命令执行时卡死。需要执行中断命令Ctrl + z 创建文件夹或文件 12$&gt; mkdir aa # 创建aa文件夹$&gt; touch aa.log # 创建aa.log文件(0字节) Linux 查看进程 1234$&gt; ps -ef | grep nginx #nobody 4554 4553 0 15:50 ? 00:00:00 nginx: worker process $&gt; ps -e | grep nginx #4553 ? 00:00:00 nginx Linux 关闭防火墙 1$&gt; sudo ufw disable 开机自启动服务 12$&gt; sudo vi /etc/rc.local# 在 exit 0 前添加所需自启动服务 安装ssh服务 1$&gt; sudo apt-get install openssh-server","tags":[{"name":"学习","slug":"学习","permalink":"http://amoyiki.com/tags/学习/"}]},{"title":"windows 下使用Virtualenv 管理Python项目","date":"2016-05-09T14:57:21.000Z","path":"2016/05/09/windows-下使用Virtualenv-管理Python项目/","text":"为什么使用virtualenv我们在使用python时可能会遇到在一台电脑同时装不同的版本Python或者装不同版本的第三方依赖。这时候就可以用virtualenv来隔绝项目之间第三方依赖。此外，virtualenv还可以把开发环境打包。一键部署到其他地方。 windows 环境安装virtualenv先决条件： 已经安装Python 已经安装pip或easy_install 安装virtual（假定按在D:\\env文件夹下）12D:\\env&gt; pip install virtualenvD:\\env&gt; pip install virtualenvwrapper 创建一个虚拟环境123D:\\env&gt; mkvirtualenv envNew python executable in D:\\workspace\\env1\\Scripts\\python.exeInstalling setuptools, pip, wheel...done. 创建完成后我们可以使用lsvirtualenv查看已经创建的虚拟环境 12345D:\\env&gt;lsvirtualenvdir /b /ad \"D:\\env\"========================================env 可以利用workon 虚拟环境名字来切环境 常用操作 激活虚拟环境 workon env 退出虚拟环境deactivate 查看当前虚拟环境安装的所有软件包pip list 结束那么，愉快的用pip去装各种各样的依赖包吧！","tags":[{"name":"Virtualenv","slug":"Virtualenv","permalink":"http://amoyiki.com/tags/Virtualenv/"},{"name":"Python","slug":"Python","permalink":"http://amoyiki.com/tags/Python/"}]},{"title":"新的开始","date":"2016-05-01T02:59:47.000Z","path":"2016/05/01/新的开始/","text":"新的开始，建立在曾经的废墟之上说来也是怪我自己脑子太二，在之前笔记本电脑内的Blog源文件没有备份就将电脑重装系统，结果就是之前的所有文章都消失得一干二净，虽然不是什么重要的文章，但毕竟是自己花心思写下的经验，丢了还是觉得蛮可惜的。所以这一次，我老老实实在GitHub上新开一个repository放我的hexo文件。以后每写一篇文章，就都要把源文件上传一次，妈妈再也不用担心换电脑没法继续写Blog了，顺带还能熟悉一些git命令，岂不美哉。 既然开了一篇杂谈，那我就再说说找工作的事吧虽然没人看，但是我还是记录一下大四找工作的一些经历和体会，以后回顾的时候也能在心里啐一句：当初怎么跟个傻子似的。 想开始找工作是在大四上的时候，虽然那个时候周围的同学都还在重复过着悠闲自在的大学生活，但是我比较杞人忧天，或许是在知乎逛多了，感觉其他大学生都屌的不行，而我以前也从未有过去实习的经历，如果还不趁着秋招公司多，到毕业时想找恐怕就难了。开始写简历的时候才发现，自己这四年来好像没什么好写的，一来学习成绩不怎么样，再来自己也没有参与过什么软件项目的开发，简直就只能白纸一张。想了好几个晚上，把自己大三开始所有写过的代码捋了一遍，挑了三个称的上开发的项目，然后一顿瞎掰开发过程。总算勉勉强强凑成了一页。接下来就是不断的参加校招，不断的递简历，不断的参加笔试面试。参加了几个比较有名的公司的校招，有的递完简历就了无音讯，有的参加完笔试就知道没机会了。不过在不断失败中，我还是攒到了一些经验，会针对面试公司可能出的问题进行复习。功夫不负有心人，虽然大公司一家都没有消息，不过海投中的一些小公司还是有给我回信。陆陆续续的面试了好几个公司，从一开始自我介绍就会声音就会发抖，到后面稍微从容的跟面试官聊一聊人生。这样的进步我觉得比我拿到offer更值得高兴。然后，我也收到了五个左右的offer，包括初创，电商，软件等公司，虽然大家都说在初创公司会得到很好的锻炼，但是我对自身技术水平还是不自信，怕到了初创公司会处于拖后腿的地位，考虑了很久，选择了一件面向企业的软件公司。工作了差不多4月，现在我也慢慢进入了社会人的角色。现在这个公司工作量不算太大，给的工资也说的过去。不过，工作内容不是自己期待中呢那样。所以，我开始思考一年以后，我该何去何从。 对未来的展望说完当下现状，就该想想以后的发展目标了。现在这个公司实在太过安逸了，每天要么处理客户问题，要么做一些重复的软件二次开发，要么就发发呆。这样让我开始担忧我的技术水平能不能在这一年有所提高，还是会比在学校更糟糕，每天下班回来基本不会再看技术书，不会在家里写一行代码。而且岗位本身就不是一个单纯的开发岗位，更像一个售后技术人员和软件定制开发人员的结合。公司同个技术小组同事虽然都很好，但是没有技术的交流，没有共同开发，大家各干各的活，这点让我很是失望。所以，我开始考虑明年跳槽，下一份工作，我决定去上海发展，在我们这个二流城市，互联网公司还是太少。机会感觉也不多。所以想去上海看看会不会有更多的机会。同时，我开始了前期的准备工作。现在做的python开发，之前在学校学的java基本上也忘了差不多，而python我也没有很深的了解，这个状态肯定无法跳槽。所以，我开始重新捡起书籍，为明年而奋斗！！","tags":[{"name":"杂谈","slug":"杂谈","permalink":"http://amoyiki.com/tags/杂谈/"}]}]